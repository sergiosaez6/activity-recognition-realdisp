{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulos necesarios. Asegurarse de poder importarlos.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pylab import rcParams\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "import tqdm as tqdm\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python\n",
      "3.6.8 (default, Aug 20 2019, 17:12:48) \n",
      "[GCC 8.3.0]\n",
      "sys.version_info(major=3, minor=6, micro=8, releaselevel='final', serial=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global activities\n",
    "activities =  [19]\n",
    "global subjects\n",
    "subjects = [1,2,3,5,8,9,10,11,13,14,16,17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 12\n",
      "Train files: 12\n",
      "TRAINING: \n",
      "['../data_reset_def/subject_01_RESET_act_19.csv', '../data_reset_def/subject_02_RESET_act_19.csv', '../data_reset_def/subject_03_RESET_act_19.csv', '../data_reset_def/subject_05_RESET_act_19.csv', '../data_reset_def/subject_08_RESET_act_19.csv', '../data_reset_def/subject_09_RESET_act_19.csv', '../data_reset_def/subject_10_RESET_act_19.csv', '../data_reset_def/subject_11_RESET_act_19.csv', '../data_reset_def/subject_13_RESET_act_19.csv', '../data_reset_def/subject_14_RESET_act_19.csv'] ...\n"
     ]
    }
   ],
   "source": [
    "# Ruta raíz\n",
    "PATH = '../data_reset_def/'\n",
    "\n",
    "quaturls = !ls -1 \"{PATH}\"\n",
    "\n",
    "\n",
    "\n",
    "quat_corr = []\n",
    "for filename in quaturls:\n",
    "    for activity in activities:\n",
    "        if(int(filename[-6:-4])==activity):\n",
    "            quat_corr.append(filename)\n",
    "\n",
    "quat_def = []\n",
    "\n",
    "i=0\n",
    "for filename in quat_corr:\n",
    "    for subject in subjects:\n",
    "        if(int(quat_corr[i][8:10])==subject):\n",
    "            quat_def.append(filename)\n",
    "    i+=1\n",
    "\n",
    "quaturls = quat_def\n",
    "\n",
    "del quat_corr, quat_def\n",
    "\n",
    "n = len(quaturls)\n",
    "\n",
    "tr_urls = quaturls\n",
    "\n",
    "print('Total files: ' + str(len(quaturls)))\n",
    "print('Train files: ' + str(len(tr_urls)))\n",
    "\n",
    "tr_fullpath = [os.path.join(PATH,s) for s in tr_urls]\n",
    "print('TRAINING: ')\n",
    "print(str(tr_fullpath[:10]) + ' ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanceo entre clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el balanceo de clases forzamos a que cada clase tenga el mismo número de muestras, así que, en este caso, limitamos el nº de muestras al que tiene el mínimo (en el futuro se hará data augmentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8928\n"
     ]
    }
   ],
   "source": [
    "n = 1e100\n",
    "for filename in tr_fullpath:\n",
    "    df = pd.read_csv(filename,sep=',') \n",
    "    \n",
    "    ind = len(df.index)\n",
    "    if(ind!=0):\n",
    "        n = min(n,ind)\n",
    "\n",
    "ind = int(n)\n",
    "if(PATH == '../data_augment/'):\n",
    "    ind_real = 24*int(n/9)\n",
    "else:\n",
    "    ind_real = int(n/9)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizamos: 992 muestras de cada actividad de cada sujeto, que se corresponde con: 19.84 segundos.\n"
     ]
    }
   ],
   "source": [
    "print('Utilizamos: '+ str(ind_real) + ' muestras de cada actividad de cada sujeto, que se corresponde con: '+ str(ind_real/50) + ' segundos.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(df, n_time_steps, step, segments, labels, label, dim, train, n_channels):\n",
    "    quat0 = df.iloc[:, 1:5][df['QUAT']=='quat0'].reset_index() # si no incluimos el reset_index(), al concatenarlos después\n",
    "    quat1 = df.iloc[:, 1:5][df['QUAT']=='quat1'].reset_index() # aparecerá un dataframe de igual nº de filas que df, pero con\n",
    "    quat2 = df.iloc[:, 1:5][df['QUAT']=='quat2'].reset_index() # NaN en las posiciones que no tienen número de cada dataframe quat,\n",
    "    quat3 = df.iloc[:, 1:5][df['QUAT']=='quat3'].reset_index() # es decir, mantiene los índices de df.\n",
    "    quat4 = df.iloc[:, 1:5][df['QUAT']=='quat4'].reset_index()\n",
    "    \n",
    "    quat = pd.concat([quat0.iloc[:, 1], quat0.iloc[:, 2], quat0.iloc[:, 3], quat0.iloc[:, 4],\n",
    "                      quat1.iloc[:, 1], quat1.iloc[:, 2], quat1.iloc[:, 3], quat1.iloc[:, 4],\n",
    "                      quat2.iloc[:, 1], quat2.iloc[:, 2], quat2.iloc[:, 3], quat2.iloc[:, 4],\n",
    "                      quat3.iloc[:, 1], quat3.iloc[:, 2], quat3.iloc[:, 3], quat3.iloc[:, 4],\n",
    "                      quat4.iloc[:, 1], quat4.iloc[:, 2], quat4.iloc[:, 3], quat4.iloc[:, 4]],\n",
    "                      axis = 1, keys = ['w0', 'x0', 'y0', 'z0', 'w1', 'x1', 'y1', 'z1', 'w2', 'x2', 'y2', 'z2', \n",
    "                                        'w3', 'x3', 'y3', 'z3', 'w4', 'x4', 'y4', 'z4'])\n",
    "    \n",
    "    del quat0, quat1, quat2, quat3, quat4\n",
    "    \n",
    "    if(n_channels == 1):\n",
    "        for i in range(0, quat.shape[0] - n_time_steps, step): # Overlap\n",
    "            # Con listas y numpy\n",
    "            segments.append([])\n",
    "            segments[dim].append(quat.iloc[i: i + n_time_steps, :].values) # Si distinguimos entre sensores\n",
    "            labels.append(label-1)\n",
    "            dim+=1\n",
    "    else:\n",
    "        n_columns = int(36/n_channels)\n",
    "        for i in range(0, quat.shape[0] - n_time_steps, step):\n",
    "            segments.append([])\n",
    "            col = 0\n",
    "            for j in range(n_channels):\n",
    "                segments[dim].append([])\n",
    "                segments[dim][j].append(quat.iloc[i:i+n_time_steps,col:col+n_columns].values)\n",
    "                col+=n_columns\n",
    "            labels.append(label-1)\n",
    "            dim+=1\n",
    "    \n",
    "        \n",
    "    del quat\n",
    "    \n",
    "    return segments, labels, dim\n",
    "\n",
    "\n",
    "def load_quat(path, lim, train, n_channels):\n",
    "\n",
    "    n_time_steps, step = 128, 1  # n_time_steps/50 segundos de actividad y pasos de step/50 segundos (de overlap)\n",
    "    \n",
    "    # Con listas\n",
    "    segments = []\n",
    "    labels = []\n",
    "    \n",
    "    m=1\n",
    "    dim = 0\n",
    "    for filename in path:\n",
    "        print(\"Reading %s (%d/%d)                                                   \"%(filename, m, len(path)), end='\\r')\n",
    "        df = pd.read_csv(filename,sep=',',names=[\"QUAT\",\"w\",\"x\",\"y\",\"z\",\"timestamp\"])\n",
    "        label = int(filename[-6:-4])\n",
    "        for i in range(len(activities)):\n",
    "            if(label==activities[i]):\n",
    "                label=i+1\n",
    "        \n",
    "        segments, labels, dim = sliding_window(df.iloc[:lim], n_time_steps, step, segments, labels, label, dim, train, n_channels)\n",
    "        \n",
    "        m+=1\n",
    "    \n",
    "    del df\n",
    "    \n",
    "    return segments, labels\n",
    "\n",
    "\n",
    "def load_train_quat(filename, lim, n_channels):\n",
    "    return load_quat(filename, lim, True, n_channels)\n",
    "\n",
    "def load_test_quat(filename, lim, n_channels):\n",
    "    return load_quat(filename, lim, False, n_channels)\n",
    "\n",
    "def get_dataset(data_path, lim, batch_size, n_time_steps, train, valid, ds, n_channels):\n",
    "    if(train):\n",
    "        segments, labels = load_train_quat(data_path, lim, n_channels)\n",
    "    else:\n",
    "        segments, labels = load_test_quat(data_path, lim, n_channels)\n",
    "    \n",
    "    print('Generating the dataset                                                   ')                                              \n",
    "    \n",
    "    array = np.asarray(segments, dtype = 'float32')\n",
    "    segments = np.reshape(array, (array.shape[0], n_channels, n_time_steps, int(20/n_channels)))\n",
    "    array = np.asarray(labels, dtype = 'int8')\n",
    "    labels = np.reshape(array, (array.shape[0], 1))\n",
    "    \n",
    "    del array\n",
    "    '''\n",
    "    # Standardization\n",
    "    if(train):\n",
    "        mean = segments.mean(axis=0)\n",
    "        std = segments.std(axis=0)\n",
    "        segments -= mean\n",
    "        segments /= std\n",
    "    else:\n",
    "        segments -= mean\n",
    "        segments /= std\n",
    "    '''\n",
    "    # Map coninous dataset to categorical (One-Hot)\n",
    "    #labels = keras.utils.to_categorical(labels, 5)\n",
    "    \n",
    "    if(train):\n",
    "        print('-'*20 + 'TRAIN' + '-'*20)\n",
    "    elif(valid):\n",
    "        print('-'*18 + 'VALIDATION' + '-'*12)\n",
    "    else:\n",
    "        print('-'*20 + 'TEST' + '-'*21)\n",
    "    \n",
    "    if(ds):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((segments, labels))\n",
    "    \n",
    "        # It's necessary to repeat our data for all epochs\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        \n",
    "        dataset = dataset.shuffle(segments.shape[0])\n",
    "        \n",
    "        print('Dataset generated                                                        ')\n",
    "        \n",
    "        return dataset, segments, labels\n",
    "    else:\n",
    "        # Shuffle in the first dimension\n",
    "        permutation = np.arange(0,segments.shape[0]-1)\n",
    "        np.random.shuffle(permutation)\n",
    "        segments = segments[permutation]\n",
    "        labels = labels[permutation]\n",
    "        print('Dataset generated                                                        ')\n",
    "        \n",
    "        return segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the dataset                                                                                          \n",
      "--------------------TRAIN--------------------\n",
      "Dataset generated                                                        \n",
      "Train dataset: \n",
      "(10367, 1, 128, 20) (10367, 1)\n"
     ]
    }
   ],
   "source": [
    "n_time_steps = 128\n",
    "dataset = False # Set to True if you want a dataset or to False if you want np.arrays\n",
    "batch_size = 16 # REAL batch_size\n",
    "n_channels = 1 # It can be 1,4 or 9\n",
    "\n",
    "if(dataset):\n",
    "    train_dataset, tr_seg, tr_lab = get_dataset(tr_fullpath, ind, batch_size, n_time_steps, True, False, dataset, n_channels)\n",
    "    \n",
    "    print('Train dataset: ')\n",
    "    print(train_dataset)\n",
    "else:\n",
    "    tr_seg, tr_lab = get_dataset(tr_fullpath, ind, batch_size, n_time_steps, True, False, dataset, n_channels)\n",
    "    print('Train dataset: ')\n",
    "    print(tr_seg.shape, tr_lab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    return tr_seg, tr_lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_optimizer():\n",
    "    return keras.optimizers.Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_12 (Reshape)         (None, None, 1000)        0         \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (None, 1000)              6003000   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2560)              2562560   \n",
      "=================================================================\n",
      "Total params: 8,565,560\n",
      "Trainable params: 8,565,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "def create_generator():\n",
    "    generator=Sequential()\n",
    "\n",
    "    units = 1000\n",
    "    \n",
    "    generator.add(Reshape((-1,units), input_shape=(1000,)))\n",
    "    \n",
    "    generator.add(GRU(units, activation='relu', reset_after = False))\n",
    "    #generator.add(Reshape((-1,units)))\n",
    "    \n",
    "    #generator.add(GRU(units, activation='relu', dropout=0.5, recurrent_dropout=0.5, reset_after = False))\n",
    "    \n",
    "    generator.add(Dense(units=tr_seg.shape[1]*tr_seg.shape[2]*tr_seg.shape[3], activation='linear'))\n",
    "    \n",
    "    generator.compile(loss='binary_crossentropy', optimizer=adam_optimizer())\n",
    "    return generator\n",
    "g=create_generator()\n",
    "g.summary()\n",
    "#keras.utils.plot_model(g, to_file=\"model.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_13 (Reshape)         (None, 1, 128, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 64, 128, 20)       1088      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 64, 32, 5)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64, 32, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 128, 32, 5)        131200    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 128, 8, 1)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128, 8, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 657,601\n",
      "Trainable params: 657,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_discriminator():\n",
    "    discriminator=Sequential()\n",
    "\n",
    "    discriminator.add(Reshape((tr_seg.shape[1],tr_seg.shape[2],tr_seg.shape[3]), input_shape=(2560,)))\n",
    "    \n",
    "    discriminator.add(Conv2D(64,kernel_size=4, strides=1, padding='same', data_format='channels_first', activation='relu'))\n",
    "    discriminator.add(MaxPooling2D(pool_size=4, strides=4, data_format='channels_first'))\n",
    "    discriminator.add(Dropout(0.5))\n",
    "       \n",
    "    discriminator.add(Conv2D(128,kernel_size=4, strides=1, padding='same', data_format='channels_first', activation='relu'))\n",
    "    discriminator.add(MaxPooling2D(pool_size=4, strides=4, data_format='channels_first'))\n",
    "    discriminator.add(Dropout(0.5))\n",
    "    \n",
    "    discriminator.add(Flatten())\n",
    "    \n",
    "    discriminator.add(Dense(units=512, activation='relu'))\n",
    "    \n",
    "    discriminator.add(Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=adam_optimizer())\n",
    "    return discriminator\n",
    "d =create_discriminator()\n",
    "d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "sequential_8 (Sequential)    (None, 2560)              8565560   \n",
      "_________________________________________________________________\n",
      "sequential_9 (Sequential)    (None, 1)                 657601    \n",
      "=================================================================\n",
      "Total params: 9,223,161\n",
      "Trainable params: 8,565,560\n",
      "Non-trainable params: 657,601\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_gan(discriminator, generator):\n",
    "    discriminator.trainable=False\n",
    "    gan_input = Input(shape=(1000,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output= discriminator(x)\n",
    "    gan= Model(inputs=gan_input, outputs=gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return gan\n",
    "gan = create_gan(d,g)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(name, ts_seg):\n",
    "    fo = open(name, \"w\")\n",
    "    head = \"QUAT,w,x,y,z,timestamp\\n\"\n",
    "    fo.seek(0,2)\n",
    "    fo.write(head)\n",
    "    fo.close()\n",
    "    \n",
    "    fo = open(name, \"a\")\n",
    "    salida = np.reshape(ts_seg, (ts_seg.shape[0]*ts_seg.shape[2], ts_seg.shape[3]))\n",
    "    \n",
    "    k = 0\n",
    "    for i in range(salida.shape[0]):\n",
    "        if(i%128==0):\n",
    "            if(i!=0):\n",
    "                k+=1\n",
    "        print('%d'%(i), end='\\r')\n",
    "        fo.write(\"quat0,\"+str(salida[i][0])+\",\"+str(salida[i][1])+\",\"+str(salida[i][2])+\",\"+str(salida[i][3])+\",\"+str(i)+\",\"+'\\n')\n",
    "        fo.write(\"quat1,\"+str(salida[i][4])+\",\"+str(salida[i][5])+\",\"+str(salida[i][6])+\",\"+str(salida[i][7])+\",\"+str(i)+\",\"+'\\n')\n",
    "        fo.write(\"quat2,\"+str(salida[i][8])+\",\"+str(salida[i][9])+\",\"+str(salida[i][10])+\",\"+str(salida[i][11])+\",\"+str(i)+\",\"+'\\n')\n",
    "        fo.write(\"quat3,\"+str(salida[i][12])+\",\"+str(salida[i][13])+\",\"+str(salida[i][14])+\",\"+str(salida[i][15])+\",\"+str(i)+\",\"+'\\n')\n",
    "        fo.write(\"quat4,\"+str(salida[i][16])+\",\"+str(salida[i][17])+\",\"+str(salida[i][18])+\",\"+str(salida[i][19])+\",\"+str(i)+\",\"+'\\n')\n",
    "        if(salida.shape[1]==36):\n",
    "            fo.write(\"quat5,\"+str(salida[i][20])+\",\"+str(salida[i][21])+\",\"+str(salida[i][22])+\",\"+str(salida[i][23])+\",\"+str(i)+\",\"+'\\n')\n",
    "            fo.write(\"quat6,\"+str(salida[i][24])+\",\"+str(salida[i][25])+\",\"+str(salida[i][26])+\",\"+str(salida[i][27])+\",\"+str(i)+\",\"+'\\n')\n",
    "            fo.write(\"quat7,\"+str(salida[i][28])+\",\"+str(salida[i][29])+\",\"+str(salida[i][30])+\",\"+str(salida[i][31])+\",\"+str(i)+\",\"+'\\n')\n",
    "            fo.write(\"quat8,\"+str(salida[i][32])+\",\"+str(salida[i][33])+\",\"+str(salida[i][34])+\",\"+str(salida[i][35])+\",\"+str(i)+\",\"+'\\n')\n",
    "        else:\n",
    "            fo.write(\"quat5,\"+str(1)+\",\"+str(0)+\",\"+str(0)+\",\"+str(0)+\",\"+str(i)+\",\"+'\\n')\n",
    "            fo.write(\"quat6,\"+str(1)+\",\"+str(0)+\",\"+str(0)+\",\"+str(0)+\",\"+str(i)+\",\"+'\\n')\n",
    "            fo.write(\"quat7,\"+str(1)+\",\"+str(0)+\",\"+str(0)+\",\"+str(0)+\",\"+str(i)+\",\"+'\\n')\n",
    "            fo.write(\"quat8,\"+str(1)+\",\"+str(0)+\",\"+str(0)+\",\"+str(0)+\",\"+str(i)+\",\"+'\\n')\n",
    "    fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 288\n",
      "5: [D loss REAL : 0.129455]  [D loss FAKE: 0.035303]  [A loss: 4.399049]\r"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def training(epochs=1, batch_size=128):\n",
    "    \n",
    "    #Loading the data\n",
    "    tr_seg, _ = load_data()\n",
    "    batch_count = tr_seg.shape[0] // batch_size\n",
    "    X_train = np.reshape(tr_seg,(tr_seg.shape[0],tr_seg.shape[2]*tr_seg.shape[3]))\n",
    "    \n",
    "    # Creating GAN\n",
    "    generator= create_generator()\n",
    "    discriminator= create_discriminator()\n",
    "    gan = create_gan(discriminator, generator)\n",
    "    \n",
    "    d_real_hist = []\n",
    "    d_fake_hist = []\n",
    "    a_hist = []\n",
    "\n",
    "    for e in range(1,epochs+1 ):\n",
    "        print(\"\\n Epoch %d\" %e)\n",
    "        for i in range (batch_count):\n",
    "        #generate  random noise as an input  to  initialize the  generator\n",
    "            noise= np.random.normal(0,1, [batch_size, 1000])\n",
    "            \n",
    "            #Pre train discriminator on  fake and real data  before starting the gan. \n",
    "            discriminator.trainable=True\n",
    "            \n",
    "            # Get a random set of  real images\n",
    "            image_batch =X_train[np.random.randint(low=0,high=X_train.shape[0],size=batch_size)]\n",
    "            y_dis=np.ones(batch_size)\n",
    "            \n",
    "            d_loss_real=discriminator.train_on_batch(image_batch, y_dis)\n",
    "            \n",
    "            # Generate fake MNIST images from noised input\n",
    "            generated_images = generator.predict(noise)\n",
    "            y_dis=np.zeros(batch_size)\n",
    "            \n",
    "            d_loss_fake=discriminator.train_on_batch(generated_images, y_dis)\n",
    "            \n",
    "            #Tricking the noised input of the Generator as real data\n",
    "            noise= np.random.normal(0,1, [batch_size, 1000])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            \n",
    "            # During the training of gan, \n",
    "            # the weights of discriminator should be fixed. \n",
    "            #We can enforce that by setting the trainable flag\n",
    "            discriminator.trainable=False\n",
    "            \n",
    "            #training  the GAN by alternating the training of the Discriminator \n",
    "            #and training the chained GAN model with Discriminator’s weights freezed.\n",
    "            # keras.utils.to_categorical(y_gen, 2)\n",
    "            a_loss=gan.train_on_batch(noise,  y_gen)\n",
    "            \n",
    "            d_real_hist.append(d_loss_real)\n",
    "            d_fake_hist.append(d_loss_fake)\n",
    "            a_hist.append(a_loss)\n",
    "            \n",
    "            log_mesg = \"%d: [D loss REAL : %f]\" % (i+1, d_loss_real)\n",
    "            log_mesg = \"%s  [D loss FAKE: %f]\" % (log_mesg, d_loss_fake)\n",
    "            log_mesg = \"%s  [A loss: %f]\" % (log_mesg, a_loss)\n",
    "            print(log_mesg, end='\\r')\n",
    "            \n",
    "        if e == 1 or e % 100 == 0:\n",
    "            name = \"./00_10_models/generated_00_10_%s.csv\"%(e)\n",
    "            write(name, np.reshape(generated_images,(-1, tr_seg.shape[1], tr_seg.shape[2], tr_seg.shape[3])))\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    return generator, discriminator, gan, d_real_hist, d_fake_hist, a_hist\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 256\n",
    "generator, discriminator, gan, d_real_hist, d_fake_hist, a_hist = training(epochs,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "generator.save('./00_10_models/generator.h5')\n",
    "print('Generator saved.')\n",
    "discriminator.save('./00_10_models/discriminator.h5')\n",
    "print('Discriminator saved.')\n",
    "gan.save('./00_10_models/GAN.h5')\n",
    "print('GAN saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count = tr_seg.shape[0] // batch_size\n",
    "\n",
    "x_ticks = np.linspace(1,epochs,num=len(d_real_hist))\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(x_ticks,d_real_hist, label='Disc. loss REAL')\n",
    "plt.plot(x_ticks,d_fake_hist, label='Disc. loss FAKE')\n",
    "plt.plot(x_ticks,a_hist, label='Adv. loss')\n",
    "plt.xlabel('Epochs', fontsize=20)\n",
    "plt.ylabel('Losses', fontsize=20)\n",
    "plt.legend(loc=1, prop={'size': 20})\n",
    "plt.tick_params(labelsize=20);\n",
    "plt.title('GAN Training', fontsize=20)\n",
    "\n",
    "plt.draw()\n",
    "plt.savefig('./00_10_models/training.tiff', bbox_inches='tight',format='tiff')\n",
    "plt.draw()\n",
    "plt.savefig('./00_10_models/training.pdf', bbox_inches='tight',format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(x_ticks,d_real_hist, label='Disc. loss REAL')\n",
    "plt.plot(x_ticks,d_fake_hist, label='Disc. loss FAKE')\n",
    "plt.xlabel('Epochs', fontsize=20)\n",
    "plt.ylabel('Losses', fontsize=20)\n",
    "plt.legend(loc=1, prop={'size': 20})\n",
    "plt.tick_params(labelsize=20);\n",
    "plt.title('GAN Training', fontsize=20)\n",
    "\n",
    "plt.draw()\n",
    "plt.savefig('./00_10_models/training_noAdv.tiff', bbox_inches='tight',format='tiff')\n",
    "plt.draw()\n",
    "plt.savefig('./00_10_models/training_noAdv.pdf', bbox_inches='tight',format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_hist = np.asarray(d_real_hist)+np.asarray(d_fake_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(x_ticks,d_hist, label='Disc. loss')\n",
    "plt.plot(x_ticks,a_hist, label='Adv. loss')\n",
    "plt.xlabel('Epochs', fontsize=20)\n",
    "plt.ylabel('Losses', fontsize=20)\n",
    "plt.legend(loc=1, prop={'size': 20})\n",
    "plt.tick_params(labelsize=20)\n",
    "plt.title('GAN Training', fontsize=20)\n",
    "\n",
    "plt.draw()\n",
    "plt.savefig('./00_10_models/training2.tiff', bbox_inches='tight',format='tiff')\n",
    "plt.draw()\n",
    "plt.savefig('./00_10_models/training2.pdf', bbox_inches='tight',format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(x_ticks,d_hist, label='Disc. loss')\n",
    "plt.xlabel('Epochs', fontsize=20)\n",
    "plt.ylabel('Losses', fontsize=20)\n",
    "plt.legend(loc=1, prop={'size': 20})\n",
    "plt.tick_params(labelsize=20)\n",
    "plt.title('GAN Training', fontsize=20)\n",
    "\n",
    "plt.draw()\n",
    "plt.savefig('./00_10_models/training2_noAdv.tiff', bbox_inches='tight',format='tiff')\n",
    "plt.draw()\n",
    "plt.savefig('./00_10_models/training2_noAdv.pdf', bbox_inches='tight',format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d_hist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d_real_hist[-1])\n",
    "print(d_fake_hist[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
