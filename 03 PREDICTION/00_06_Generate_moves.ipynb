{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN para 33 actividades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separación de sets\n",
    "Primero separamos los quaternions en sets de entrenamiento, validación y test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulos necesarios. Asegurarse de poder importarlos.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pylab import rcParams\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python\n",
      "3.6.8 (default, Aug 20 2019, 17:12:48) \n",
      "[GCC 8.3.0]\n",
      "sys.version_info(major=3, minor=6, micro=8, releaselevel='final', serial=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos uso de los ficheros que contiene los cuaterniones separados por sujetos y actividad.\n",
    "\n",
    "Definimos una serie de funciones para la lectura de los datos y finalmente realizamos el dataset (así leemos los datos en el entrenamiento \"al vuelo\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta raíz\n",
    "PATH = '../data_reset_def'\n",
    "\n",
    "# Ruta de los ceckpoints\n",
    "CKPATH = PATH + '/checkpoints'\n",
    "\n",
    "quaturls = !ls -1 \"{PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "global activities\n",
    "activities =  [19]\n",
    "global subjects\n",
    "subjects = [1,2,3,5,8,9,10,11,13,14,16,17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "quat_corr = []\n",
    "for filename in quaturls:\n",
    "    for activity in activities:\n",
    "        if(int(filename[-6:-4])==activity):\n",
    "            quat_corr.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "quat_def = []\n",
    "\n",
    "i=0\n",
    "for filename in quat_corr:\n",
    "    for subject in subjects:\n",
    "        if(int(quat_corr[i][8:10])==subject):\n",
    "            quat_def.append(filename)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "quaturls = quat_def\n",
    "\n",
    "del quat_corr, quat_def\n",
    "\n",
    "#print(quaturls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación por sujetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 12\n",
      "Train files: 9\n",
      "Validation files: 2\n",
      "Test files: 1\n"
     ]
    }
   ],
   "source": [
    "# Shuffle de los datos\n",
    "n = len(quaturls)\n",
    "\n",
    "tr_urls = []\n",
    "va_urls = []\n",
    "ts_urls = []\n",
    "\n",
    "i=0\n",
    "for filename in quaturls:\n",
    "    if(int(quaturls[i][8:10])==2 or int(quaturls[i][8:10])==10):\n",
    "        va_urls.append(filename)\n",
    "    elif(int(quaturls[i][8:10])==16):\n",
    "        ts_urls.append(filename)\n",
    "    else:\n",
    "        tr_urls.append(filename)\n",
    "    i+=1\n",
    "\n",
    "print('Total files: ' + str(len(quaturls)))\n",
    "print('Train files: ' + str(len(tr_urls)))\n",
    "print('Validation files: ' + str(len(va_urls)))\n",
    "print('Test files: ' + str(len(ts_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING: \n",
      "['../data_reset_def/subject_01_RESET_act_19.csv', '../data_reset_def/subject_03_RESET_act_19.csv', '../data_reset_def/subject_05_RESET_act_19.csv', '../data_reset_def/subject_08_RESET_act_19.csv', '../data_reset_def/subject_09_RESET_act_19.csv', '../data_reset_def/subject_11_RESET_act_19.csv', '../data_reset_def/subject_13_RESET_act_19.csv', '../data_reset_def/subject_14_RESET_act_19.csv', '../data_reset_def/subject_17_RESET_act_19.csv']\n",
      "VALIDATION: \n",
      "['../data_reset_def/subject_02_RESET_act_19.csv', '../data_reset_def/subject_10_RESET_act_19.csv']\n",
      "TEST: \n",
      "['../data_reset_def/subject_16_RESET_act_19.csv']\n"
     ]
    }
   ],
   "source": [
    "tr_fullpath = [os.path.join(PATH,s) for s in tr_urls]\n",
    "print('TRAINING: ')\n",
    "print(tr_fullpath)\n",
    "va_fullpath = [os.path.join(PATH,s) for s in va_urls]\n",
    "print('VALIDATION: ')\n",
    "print(va_fullpath)\n",
    "ts_fullpath = [os.path.join(PATH,s) for s in ts_urls]\n",
    "print('TEST: ')\n",
    "print(ts_fullpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con listas y numpy (la más pesada de todas, pero seguros en que mantiene la estructura de los datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(df, n_time_steps, step, segments, labels, label, dim, train, n_channels):\n",
    "    quat0 = df.iloc[:, 1:5][df['QUAT']=='quat0'].reset_index() # si no incluimos el reset_index(), al concatenarlos después\n",
    "    quat1 = df.iloc[:, 1:5][df['QUAT']=='quat1'].reset_index() # aparecerá un dataframe de igual nº de filas que df, pero con\n",
    "    quat2 = df.iloc[:, 1:5][df['QUAT']=='quat2'].reset_index() # NaN en las posiciones que no tienen número de cada dataframe quat,\n",
    "    quat3 = df.iloc[:, 1:5][df['QUAT']=='quat3'].reset_index() # es decir, mantiene los índices de df.\n",
    "    quat4 = df.iloc[:, 1:5][df['QUAT']=='quat4'].reset_index()\n",
    "    #quat5 = df.iloc[:, 1:5][df['QUAT']=='quat5'].reset_index()\n",
    "    #quat6 = df.iloc[:, 1:5][df['QUAT']=='quat6'].reset_index()\n",
    "    #quat7 = df.iloc[:, 1:5][df['QUAT']=='quat7'].reset_index()\n",
    "    #quat8 = df.iloc[:, 1:5][df['QUAT']=='quat8'].reset_index()\n",
    "    \n",
    "    quat = pd.concat([quat0.iloc[:, 1], quat0.iloc[:, 2], quat0.iloc[:, 3], quat0.iloc[:, 4],\n",
    "                      quat1.iloc[:, 1], quat1.iloc[:, 2], quat1.iloc[:, 3], quat1.iloc[:, 4],\n",
    "                      quat2.iloc[:, 1], quat2.iloc[:, 2], quat2.iloc[:, 3], quat2.iloc[:, 4],\n",
    "                      quat3.iloc[:, 1], quat3.iloc[:, 2], quat3.iloc[:, 3], quat3.iloc[:, 4],\n",
    "                      quat4.iloc[:, 1], quat4.iloc[:, 2], quat4.iloc[:, 3], quat4.iloc[:, 4]],\n",
    "                      #quat5.iloc[:, 1], quat5.iloc[:, 2], quat5.iloc[:, 3], quat5.iloc[:, 4],\n",
    "                      #quat6.iloc[:, 1], quat6.iloc[:, 2], quat6.iloc[:, 3], quat6.iloc[:, 4],\n",
    "                      #quat7.iloc[:, 1], quat7.iloc[:, 2], quat7.iloc[:, 3], quat7.iloc[:, 4],\n",
    "                      #quat8.iloc[:, 1], quat8.iloc[:, 2], quat8.iloc[:, 3], quat8.iloc[:, 4]],\n",
    "                      axis = 1, keys = ['w0', 'x0', 'y0', 'z0', 'w1', 'x1', 'y1', 'z1', 'w2', 'x2', 'y2', 'z2', \n",
    "                                        'w3', 'x3', 'y3', 'z3', 'w4', 'x4', 'y4', 'z4']) #'w5', 'x5', 'y5', 'z5',\n",
    "                                        #'w6', 'x6', 'y6', 'z6', 'w7', 'x7', 'y7', 'z7', 'w8', 'x8', 'y8', 'z8'])\n",
    "    del quat0, quat1, quat2, quat3, quat4 #quat5, quat6, quat7, quat8\n",
    "    \n",
    "    if(train==False):\n",
    "        step = n_time_steps\n",
    "    \n",
    "    if(n_channels == 1):\n",
    "        for i in range(0, quat.shape[0] - n_time_steps, step): # Overlap\n",
    "            # Con listas y numpy\n",
    "            segments.append([])\n",
    "            segments[dim].append(quat.iloc[i: i + n_time_steps, :].values) # Si distinguimos entre sensores\n",
    "            labels.append(label-1)\n",
    "            dim+=1\n",
    "    else:\n",
    "        n_columns = int(36/n_channels)\n",
    "        for i in range(0, quat.shape[0] - n_time_steps, step):\n",
    "            segments.append([])\n",
    "            col = 0\n",
    "            for j in range(n_channels):\n",
    "                segments[dim].append([])\n",
    "                segments[dim][j].append(quat.iloc[i:i+n_time_steps,col:col+n_columns].values)\n",
    "                col+=n_columns\n",
    "            labels.append(label-1)\n",
    "            dim+=1\n",
    "    \n",
    "        \n",
    "    del quat\n",
    "    \n",
    "    return segments, labels, dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quat(path, train, n_channels):\n",
    "\n",
    "    n_time_steps, step = 1, 1  # n_time_steps/50 segundos de actividad y pasos de step/50 segundos (de overlap)\n",
    "    \n",
    "    # Con listas\n",
    "    segments = []\n",
    "    labels = []\n",
    "    \n",
    "    i=1\n",
    "    dim = 0\n",
    "    for filename in path:\n",
    "        print(\"Reading %s (%d/%d)                                                   \"%(filename, i, len(path)), end='\\r')\n",
    "        df = pd.read_csv(filename,sep=',')\n",
    "        label = int(filename[-6:-4])\n",
    "        for i in range(len(activities)):\n",
    "            if(label==activities[i]):\n",
    "                label=i+1\n",
    "        \n",
    "        segments, labels, dim = sliding_window(df, n_time_steps, step, segments, labels, label, dim, train, n_channels)\n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    del df\n",
    "    \n",
    "    return segments, labels\n",
    "\n",
    "\n",
    "def load_train_quat(filename, n_channels):\n",
    "    return load_quat(filename, True, n_channels)\n",
    "\n",
    "def load_test_quat(filename, n_channels):\n",
    "    return load_quat(filename, False, n_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data_path, batch_size, n_time_steps, train, valid, ds, n_channels):\n",
    "    if(train):\n",
    "        segments, labels = load_train_quat(data_path, n_channels)\n",
    "    else:\n",
    "        segments, labels = load_test_quat(data_path, n_channels)\n",
    "    \n",
    "    print('Generating the dataset                                                   ')                                              \n",
    "    \n",
    "    array = np.asarray(segments, dtype = 'float32')\n",
    "    segments = np.reshape(array, (array.shape[0], n_channels, n_time_steps, int(20/n_channels)))\n",
    "    array = np.asarray(labels, dtype = 'int8')\n",
    "    labels = np.reshape(array, (array.shape[0], 1))\n",
    "    \n",
    "    del array\n",
    "    \n",
    "    # Map coninous dataset to categorical (One-Hot)\n",
    "    labels = keras.utils.to_categorical(labels, 17)\n",
    "    \n",
    "    if(train):\n",
    "        print('-'*20 + 'TRAIN' + '-'*20)\n",
    "    elif(valid):\n",
    "        print('-'*18 + 'VALIDATION' + '-'*12)\n",
    "    else:\n",
    "        print('-'*20 + 'TEST' + '-'*21)\n",
    "    \n",
    "    if(ds):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((segments, labels))\n",
    "    \n",
    "        # It's necessary to repeat our data for all epochs\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        \n",
    "        dataset = dataset.shuffle(segments.shape[0])\n",
    "        \n",
    "        print('Dataset generated                                                        ')\n",
    "        \n",
    "        return dataset, segments, labels\n",
    "    else:\n",
    "        # Shuffle in the first dimension\n",
    "        #permutation = np.arange(0,segments.shape[0]-1)\n",
    "        #np.random.shuffle(permutation)\n",
    "        #segments = segments[permutation]\n",
    "        #labels = labels[permutation]\n",
    "        print('Dataset generated                                                        ')\n",
    "        \n",
    "        return segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the dataset                                                                                        \n",
      "--------------------TRAIN--------------------\n",
      "Dataset generated                                                        \n",
      "Generating the dataset                                                                                        \n",
      "------------------VALIDATION------------\n",
      "Dataset generated                                                        \n",
      "Generating the dataset                                                                                        \n",
      "--------------------TEST---------------------\n",
      "Dataset generated                                                        \n",
      "Train dataset: \n",
      "(10927, 1, 1, 20)\n",
      "Validation dataset: \n",
      "(2222, 1, 1, 20)\n",
      "Test dataset: \n",
      "(1050, 1, 1, 20)\n"
     ]
    }
   ],
   "source": [
    "n_time_steps = 1\n",
    "dataset = False # Set to True if you want a dataset or to False if you want np.arrays\n",
    "batch_size = 1 # REAL batch_size\n",
    "n_channels = 1 # It can be 1,4 or 9\n",
    "\n",
    "if(dataset):\n",
    "    train_dataset, tr_seg, tr_lab = get_dataset(tr_fullpath, ind, batch_size, n_time_steps, True, False, dataset, n_channels)\n",
    "    valid_dataset, va_seg, va_lab = get_dataset(va_fullpath, ind, batch_size, n_time_steps, False, True, dataset, n_channels)\n",
    "    test_dataset, ts_seg, ts_lab = get_dataset(ts_fullpath, ind, batch_size, n_time_steps, False, False, dataset, n_channels)\n",
    "    \n",
    "    print('Train dataset: ')\n",
    "    print(train_dataset)\n",
    "    print('Validation dataset: ')\n",
    "    print(valid_dataset)\n",
    "    print('Test dataset: ')\n",
    "    print(test_dataset)\n",
    "else:\n",
    "    tr_seg, _ = get_dataset(tr_fullpath, batch_size, n_time_steps, True, False, dataset, n_channels)\n",
    "    va_seg, _ = get_dataset(va_fullpath, batch_size, n_time_steps, False, True, dataset, n_channels)\n",
    "    ts_seg, _ = get_dataset(ts_fullpath, batch_size, n_time_steps, False, False, dataset, n_channels)\n",
    "    print('Train dataset: ')\n",
    "    print(tr_seg.shape)\n",
    "    print('Validation dataset: ')\n",
    "    print(va_seg.shape)\n",
    "    print('Test dataset: ')\n",
    "    print(ts_seg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_seg = np.reshape(tr_seg,(tr_seg.shape[0],tr_seg.shape[2],tr_seg.shape[3]))\n",
    "va_seg = np.reshape(va_seg,(va_seg.shape[0],va_seg.shape[2],va_seg.shape[3]))\n",
    "ts_seg = np.reshape(ts_seg,(ts_seg.shape[0],ts_seg.shape[2],ts_seg.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: \n",
      "(10927, 1, 20)\n",
      "Validation dataset: \n",
      "(2222, 1, 20)\n",
      "Test dataset: \n",
      "(1050, 1, 20)\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset: ')\n",
    "print(tr_seg.shape)\n",
    "print('Validation dataset: ')\n",
    "print(va_seg.shape)\n",
    "print('Test dataset: ')\n",
    "print(ts_seg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_seg = np.reshape(tr_seg,(tr_seg.shape[0],tr_seg.shape[2]))\n",
    "va_seg = np.reshape(va_seg,(va_seg.shape[0],va_seg.shape[2]))\n",
    "ts_seg = np.reshape(ts_seg,(ts_seg.shape[0],ts_seg.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "tr_seg = scaler.fit_transform(tr_seg)\n",
    "va_seg = scaler.fit_transform(va_seg)\n",
    "ts_seg = scaler.fit_transform(ts_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_seg = np.reshape(tr_seg,(tr_seg.shape[0], 1, tr_seg.shape[1]))\n",
    "va_seg = np.reshape(va_seg,(va_seg.shape[0], 1, va_seg.shape[1]))\n",
    "ts_seg = np.reshape(ts_seg,(ts_seg.shape[0], 1, ts_seg.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y(seg):\n",
    "    Y = []\n",
    "    for i in range(1, seg.shape[0]):\n",
    "        Y.append(seg[i,0,:])\n",
    "    return np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.reshape(tr_seg[:-1],(tr_seg.shape[0]-1, tr_seg.shape[1], tr_seg.shape[2]))\n",
    "#trainY = np.reshape(tr_seg[1:],(tr_seg.shape[0]-1, tr_seg.shape[1], tr_seg.shape[2]))\n",
    "trainY = create_y(tr_seg)\n",
    "\n",
    "validX = np.reshape(va_seg[:-1],(va_seg.shape[0]-1, va_seg.shape[1], va_seg.shape[2]))\n",
    "#validY = np.reshape(va_seg[1:],(va_seg.shape[0]-1, va_seg.shape[1], va_seg.shape[2]))\n",
    "validY = create_y(va_seg)\n",
    "\n",
    "testX = np.reshape(ts_seg[:-1],(ts_seg.shape[0]-1, ts_seg.shape[1], ts_seg.shape[2]))\n",
    "#testY = np.reshape(ts_seg[1:],(ts_seg.shape[0]-1, ts_seg.shape[1], ts_seg.shape[2]))\n",
    "testY = create_y(ts_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10926, 1, 20) (10926, 20)\n",
      "(2221, 1, 20) (2221, 20)\n",
      "(1049, 1, 20) (1049, 20)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape, trainY.shape)\n",
    "print(validX.shape, validY.shape)\n",
    "print(testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03213006 0.5755148  0.07074335 0.29704988 0.15412912 0.8785968\n",
      "  0.50078833 0.3276651  0.13070309 0.8118694  0.54035205 0.401704\n",
      "  0.34697625 0.60212797 0.808611   0.87900615 0.83248734 0.30239835\n",
      "  0.2578796  0.2831363 ]]\n",
      "[0.03213006 0.5755148  0.07074335 0.29704988 0.15412912 0.8785968\n",
      " 0.50078833 0.3276651  0.13070309 0.8118694  0.54035205 0.401704\n",
      " 0.34697625 0.60212797 0.808611   0.87900615 0.83248734 0.30239835\n",
      " 0.2578796  0.2831363 ]\n"
     ]
    }
   ],
   "source": [
    "print(trainX[1])\n",
    "print(trainY[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "RNN cargada y compilada.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "rnn = load_model('./00_06_models/predict_rnn.h5')\n",
    "# Compilamos el modelo\n",
    "rnn.compile(optimizer=keras.optimizers.RMSprop(lr=1e-4), loss='mean_squared_error', metrics=['accuracy'])\n",
    "print('RNN cargada y compilada.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1, 20)\n"
     ]
    }
   ],
   "source": [
    "tX = testX[:initial]\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10\r"
     ]
    }
   ],
   "source": [
    "t=10\n",
    "for i in range(t):\n",
    "    print('%d/%d'%(i+1,t), end='\\r')\n",
    "    prediction=rnn.predict(np.reshape(tX[-128:],(128,1,20)))\n",
    "    pred=np.reshape(prediction,(128,1,prediction.shape[1]))\n",
    "    tX=np.concatenate((tX,pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536, 1, 20)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 1, 20)\n"
     ]
    }
   ],
   "source": [
    "predict = tX[initial:]\n",
    "print(predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=np.reshape(predict,(predict.shape[0],predict.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = scaler.inverse_transform(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lo escribimos a fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(name, ts_seg):\n",
    "    fo = open(name, \"w\")\n",
    "    head = \"QUAT,w,x,y,z,timestamp\\n\"\n",
    "    fo.seek(0,2)\n",
    "    fo.write(head)\n",
    "    fo.close()\n",
    "    \n",
    "    fo = open(name, \"a\")\n",
    "    #salida = np.reshape(ts_seg, (ts_seg.shape[0]*ts_seg.shape[2], ts_seg.shape[3]))\n",
    "    salida = ts_seg\n",
    "    \n",
    "    k = 0\n",
    "    for i in range(salida.shape[0]):\n",
    "        if(i%128==0):\n",
    "            if(i!=0):\n",
    "                k+=1\n",
    "        print('%d'%(i), end='\\r')\n",
    "        fo.write(\"quat0,\"+str(salida[i][0])+\",\"+str(salida[i][1])+\",\"+str(salida[i][2])+\",\"+str(salida[i][3])+\",\"+str(i)+\",\"+'\\n')\n",
    "        fo.write(\"quat1,\"+str(salida[i][4])+\",\"+str(salida[i][5])+\",\"+str(salida[i][6])+\",\"+str(salida[i][7])+\",\"+str(i)+\",\"+'\\n')\n",
    "        fo.write(\"quat2,\"+str(salida[i][8])+\",\"+str(salida[i][9])+\",\"+str(salida[i][10])+\",\"+str(salida[i][11])+\",\"+str(i)+\",\"+'\\n')\n",
    "        fo.write(\"quat3,\"+str(salida[i][12])+\",\"+str(salida[i][13])+\",\"+str(salida[i][14])+\",\"+str(salida[i][15])+\",\"+str(i)+\",\"+'\\n')\n",
    "        fo.write(\"quat4,\"+str(salida[i][16])+\",\"+str(salida[i][17])+\",\"+str(salida[i][18])+\",\"+str(salida[i][19])+\",\"+str(i)+\",\"+'\\n')\n",
    "        if(salida.shape[1]==36):\n",
    "            fo.write(\"quat5,\"+str(salida[i][20])+\",\"+str(salida[i][21])+\",\"+str(salida[i][22])+\",\"+str(salida[i][23])+\",\"+str(i)+\",\"+'\\n')\n",
    "            fo.write(\"quat6,\"+str(salida[i][24])+\",\"+str(salida[i][25])+\",\"+str(salida[i][26])+\",\"+str(salida[i][27])+\",\"+str(i)+\",\"+'\\n')\n",
    "            fo.write(\"quat7,\"+str(salida[i][28])+\",\"+str(salida[i][29])+\",\"+str(salida[i][30])+\",\"+str(salida[i][31])+\",\"+str(i)+\",\"+'\\n')\n",
    "            fo.write(\"quat8,\"+str(salida[i][32])+\",\"+str(salida[i][33])+\",\"+str(salida[i][34])+\",\"+str(salida[i][35])+\",\"+str(i)+\",\"+'\\n')\n",
    "        else:\n",
    "            fo.write(\"quat5,\"+str(1)+\",\"+str(0)+\",\"+str(0)+\",\"+str(0)+\",\"+str(i)+\",\"+'\\n')\n",
    "            fo.write(\"quat6,\"+str(1)+\",\"+str(0)+\",\"+str(0)+\",\"+str(0)+\",\"+str(i)+\",\"+'\\n')\n",
    "            fo.write(\"quat7,\"+str(1)+\",\"+str(0)+\",\"+str(0)+\",\"+str(0)+\",\"+str(i)+\",\"+'\\n')\n",
    "            fo.write(\"quat8,\"+str(1)+\",\"+str(0)+\",\"+str(0)+\",\"+str(0)+\",\"+str(i)+\",\"+'\\n')\n",
    "    fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279\r"
     ]
    }
   ],
   "source": [
    "name = \"./00_06_models/RESULT_GEN_1MUESTRA_00_06.csv\"\n",
    "write(name, predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
