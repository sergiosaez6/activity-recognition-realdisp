{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FHBeGLPWzuj"
      },
      "source": [
        "# Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHZ-ziO2Wzuk"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8gqwOzRWzul"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "from six.moves import cPickle as pickle\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import csv\n",
        "from pylab import rcParams\n",
        "from scipy import stats\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "import os as os\n",
        "from mpl_toolkits.axes_grid1 import host_subplot\n",
        "import mpl_toolkits.axisartist as AA\n",
        "\n",
        "# Config the matplotlib backend as plotting inline in IPython\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTWUW1UoWzul",
        "outputId": "55f20493-8716-40f4-c6e1-f71c256f7e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/bin/python\n",
            "3.6.8 (default, Aug 20 2019, 17:12:48) \n",
            "[GCC 8.3.0]\n",
            "sys.version_info(major=3, minor=6, micro=8, releaselevel='final', serial=0)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "print(sys.version)\n",
        "print(sys.version_info)\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNgkyeCTWzum",
        "outputId": "3ba9df92-ded8-45c8-8d67-e2a5a84125fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.2.4-tf'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keras.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQvKtTKTWzum"
      },
      "source": [
        "# Dataset generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHJA2Ic4Wzun"
      },
      "source": [
        "I make use of the files that contain the quaternions differnatiated per subject and activity.\n",
        "\n",
        "I define a number of functions to read the data and finally generated the dataset 'on the fly'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHoaFMt7Wzun"
      },
      "outputs": [],
      "source": [
        "# Root path\n",
        "PATH = '../data_reset_def_raworientation'\n",
        "\n",
        "#Checkpoints path\n",
        "CKPATH = PATH + '/checkpoints'\n",
        "\n",
        "quaturls = !ls -1 \"{PATH}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKSgE1pPWzun"
      },
      "outputs": [],
      "source": [
        "n_act = 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w1X1bGZWzun"
      },
      "outputs": [],
      "source": [
        "global activities\n",
        "activities = [9,10,11,12,13,19,20,21,24,25,31]\n",
        "activities = activities[:n_act]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTnTGGxuWzun"
      },
      "outputs": [],
      "source": [
        "quat_corr = []\n",
        "for filename in quaturls:\n",
        "    for activity in activities:\n",
        "        if(int(filename[-6:-4])==activity):\n",
        "            quat_corr.append(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTyXIYZOWzun"
      },
      "outputs": [],
      "source": [
        "global subjects\n",
        "subjects = [1,2,3,5,8,9,10,11,13,14,16,17]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxYQtb9rWzun"
      },
      "outputs": [],
      "source": [
        "quaturls = []\n",
        "\n",
        "i=0\n",
        "for filename in quat_corr:\n",
        "    for subject in subjects:\n",
        "        if(int(quat_corr[i][8:10])==subject):\n",
        "            quaturls.append(filename)\n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d17gaBO9Wzun"
      },
      "source": [
        "### Differentiating per subject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaITw7ogWzun",
        "outputId": "c040bbd0-1e66-45a7-e1d4-7167b7fdaaf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total files: 132\n"
          ]
        }
      ],
      "source": [
        "print('Total files: ' + str(len(quaturls)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akSPQNLCWzun",
        "outputId": "f3fc91ed-8644-4b31-859c-d892dfd5c503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA: \n",
            "['../data_reset_def_raworientation/subject_01_RESET_RAW_act_09.csv', '../data_reset_def_raworientation/subject_01_RESET_RAW_act_10.csv', '../data_reset_def_raworientation/subject_01_RESET_RAW_act_11.csv', '../data_reset_def_raworientation/subject_01_RESET_RAW_act_12.csv', '../data_reset_def_raworientation/subject_01_RESET_RAW_act_13.csv', '../data_reset_def_raworientation/subject_01_RESET_RAW_act_19.csv', '../data_reset_def_raworientation/subject_01_RESET_RAW_act_20.csv', '../data_reset_def_raworientation/subject_01_RESET_RAW_act_21.csv', '../data_reset_def_raworientation/subject_01_RESET_RAW_act_24.csv', '../data_reset_def_raworientation/subject_01_RESET_RAW_act_25.csv']\n"
          ]
        }
      ],
      "source": [
        "data_fullpath = [os.path.join(PATH,s) for s in quaturls]\n",
        "print('DATA: ')\n",
        "print(data_fullpath[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta29dxgqWzun",
        "outputId": "c3993236-079d-4a62-e109-5d563860ccb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nk=0\\nfor i in range(12):\\n    print(data_fullpath[k:k+11])\\n    print('\\n')\\n    k+=11\\n\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "k=0\n",
        "for i in range(12):\n",
        "    print(data_fullpath[k:k+11])\n",
        "    print('\\n')\n",
        "    k+=11\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ew7v3VkWzuo"
      },
      "source": [
        "### Class balancing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXqzrV_CWzuo"
      },
      "source": [
        "For class balancing we make each class have the same number of samples, so we limit it to the minimum-class number (there will be data augmentation afterwards):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl6Jwjp1Wzuo",
        "outputId": "7b6c0ac6-2037-410f-a29c-d026da7d30d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../data_reset_def_raworientation/subject_16_RESET_RAW_act_24.csv\r"
          ]
        }
      ],
      "source": [
        "n = 1e1000\n",
        "for filename in data_fullpath:\n",
        "    df = pd.read_csv(filename,sep=',')\n",
        "\n",
        "    ind = len(df.index)\n",
        "    if(ind!=0 and ind<n):\n",
        "        n = ind\n",
        "        print(filename, end='\\r')\n",
        "\n",
        "ind_tr = int(n)\n",
        "if(PATH == '../data_augment'):\n",
        "    ind_real = 24*int(n/9)\n",
        "else:\n",
        "    ind_real = int(n/9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjOBWjVsWzuo"
      },
      "source": [
        "AsÃ­ que:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azwyLMyhWzuo",
        "outputId": "b32b2f0e-8bd5-4b9c-ce3a-234f71113252"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tenemos: 401 muestras de cada actividad de cada sujeto, que se corresponde con: 8.02 segundos.\n"
          ]
        }
      ],
      "source": [
        "print('Tenemos: '+ str(ind_real) + ' muestras de cada actividad de cada sujeto, que se corresponde con: '+ str(ind_real/50) + ' segundos.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbdodLhXWzuo",
        "outputId": "af3391e4-30a4-42de-ecf9-34c6da6145e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3609\n"
          ]
        }
      ],
      "source": [
        "print(ind_tr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMrScuzHWzuo"
      },
      "source": [
        "### Differentiating per subject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGPlIfZvWzuo"
      },
      "outputs": [],
      "source": [
        "s1, s2, s3, s5, s8, s9, s10, s11, s13, s14, s16, s17 = [], [], [], [], [], [], [], [], [], [], [], []\n",
        "\n",
        "i=0\n",
        "for filename in data_fullpath:\n",
        "    if(int(data_fullpath[i][41:43])==1):\n",
        "        s1.append(filename)\n",
        "    elif(int(data_fullpath[i][41:43])==2):\n",
        "        s2.append(filename)\n",
        "    elif(int(data_fullpath[i][41:43])==3):\n",
        "        s3.append(filename)\n",
        "    elif(int(data_fullpath[i][41:43])==5):\n",
        "        s5.append(filename)\n",
        "    elif(int(data_fullpath[i][41:43])==8):\n",
        "        s8.append(filename)\n",
        "    elif(int(data_fullpath[i][41:43])==9):\n",
        "        s9.append(filename)\n",
        "    elif(int(data_fullpath[i][41:43])==10):\n",
        "        s10.append(filename)\n",
        "    elif(int(data_fullpath[i][41:43])==11):\n",
        "        s11.append(filename)\n",
        "    elif(int(data_fullpath[i][41:43])==13):\n",
        "        s13.append(filename)\n",
        "    elif(int(data_fullpath[i][41:43])==14):\n",
        "        s14.append(filename)\n",
        "    elif(int(data_fullpath[i][41:43])==16):\n",
        "        s16.append(filename)\n",
        "    elif(int(data_fullpath[i][41:43])==17):\n",
        "        s17.append(filename)\n",
        "\n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8z18aJUWzuo",
        "outputId": "23639616-5294-42ff-a45e-150f351999f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['../data_reset_def_raworientation/subject_01_RESET_RAW_act_09.csv', '../data_reset_def_raworientation/subject_01_RESET_RAW_act_10.csv'] ['../data_reset_def_raworientation/subject_02_RESET_RAW_act_09.csv', '../data_reset_def_raworientation/subject_02_RESET_RAW_act_10.csv'] ['../data_reset_def_raworientation/subject_03_RESET_RAW_act_09.csv', '../data_reset_def_raworientation/subject_03_RESET_RAW_act_10.csv'] ['../data_reset_def_raworientation/subject_05_RESET_RAW_act_09.csv', '../data_reset_def_raworientation/subject_05_RESET_RAW_act_10.csv'] ['../data_reset_def_raworientation/subject_08_RESET_RAW_act_09.csv', '../data_reset_def_raworientation/subject_08_RESET_RAW_act_10.csv'] ['../data_reset_def_raworientation/subject_09_RESET_RAW_act_09.csv', '../data_reset_def_raworientation/subject_09_RESET_RAW_act_10.csv'] ['../data_reset_def_raworientation/subject_10_RESET_RAW_act_09.csv', '../data_reset_def_raworientation/subject_10_RESET_RAW_act_10.csv'] ['../data_reset_def_raworientation/subject_11_RESET_RAW_act_09.csv', '../data_reset_def_raworientation/subject_11_RESET_RAW_act_10.csv'] ['../data_reset_def_raworientation/subject_13_RESET_RAW_act_09.csv', '../data_reset_def_raworientation/subject_13_RESET_RAW_act_10.csv'] ['../data_reset_def_raworientation/subject_14_RESET_RAW_act_09.csv', '../data_reset_def_raworientation/subject_14_RESET_RAW_act_10.csv'] ['../data_reset_def_raworientation/subject_16_RESET_RAW_act_09.csv', '../data_reset_def_raworientation/subject_16_RESET_RAW_act_10.csv'] ['../data_reset_def_raworientation/subject_17_RESET_RAW_act_09.csv', '../data_reset_def_raworientation/subject_17_RESET_RAW_act_10.csv']\n"
          ]
        }
      ],
      "source": [
        "print(s1[:2], s2[:2], s3[:2], s5[:2], s8[:2], s9[:2], s10[:2], s11[:2], s13[:2], s14[:2], s16[:2], s17[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJkzNNl1Wzuo"
      },
      "source": [
        "### Working with lists and numpy (heavier but helps to keep dimensions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jltrB24KWzuo"
      },
      "outputs": [],
      "source": [
        "def sliding_window(df, n_time_steps, step, segments, labels, label, dim, train, n_channels):\n",
        "    quat0 = df.iloc[:, 1:5][df['QUAT']=='quat0'].reset_index() # si no incluimos el reset_index(), al concatenarlos despuÃ©s\n",
        "    quat1 = df.iloc[:, 1:5][df['QUAT']=='quat1'].reset_index() # aparecerÃ¡ un dataframe de igual nÂº de filas que df, pero con\n",
        "    quat2 = df.iloc[:, 1:5][df['QUAT']=='quat2'].reset_index() # NaN en las posiciones que no tienen nÃºmero de cada dataframe quat,\n",
        "    quat3 = df.iloc[:, 1:5][df['QUAT']=='quat3'].reset_index() # es decir, mantiene los Ã­ndices de df.\n",
        "    quat4 = df.iloc[:, 1:5][df['QUAT']=='quat4'].reset_index()\n",
        "\n",
        "    quat = pd.concat([quat0.iloc[:, 1], quat0.iloc[:, 2], quat0.iloc[:, 3], quat0.iloc[:, 4],\n",
        "                      quat1.iloc[:, 1], quat1.iloc[:, 2], quat1.iloc[:, 3], quat1.iloc[:, 4],\n",
        "                      quat2.iloc[:, 1], quat2.iloc[:, 2], quat2.iloc[:, 3], quat2.iloc[:, 4],\n",
        "                      quat3.iloc[:, 1], quat3.iloc[:, 2], quat3.iloc[:, 3], quat3.iloc[:, 4],\n",
        "                      quat4.iloc[:, 1], quat4.iloc[:, 2], quat4.iloc[:, 3], quat4.iloc[:, 4]],\n",
        "                      axis = 1, keys = ['w0', 'x0', 'y0', 'z0', 'w1', 'x1', 'y1', 'z1', 'w2', 'x2', 'y2', 'z2',\n",
        "                                        'w3', 'x3', 'y3', 'z3', 'w4', 'x4', 'y4', 'z4'])\n",
        "    del quat0, quat1, quat2, quat3, quat4\n",
        "\n",
        "    if(train==False):\n",
        "        step = n_time_steps//2\n",
        "\n",
        "    if(n_channels == 1):\n",
        "        for i in range(0, quat.shape[0] - n_time_steps, step): # Overlap\n",
        "            # Con listas y numpy\n",
        "            segments.append([])\n",
        "            segments[dim].append(quat.iloc[i: i + n_time_steps, :].values) # Si distinguimos entre sensores\n",
        "            labels.append(int(label-1))\n",
        "            dim+=1\n",
        "    else:\n",
        "        n_columns = int(36/n_channels)\n",
        "        for i in range(0, quat.shape[0] - n_time_steps, step):\n",
        "            segments.append([])\n",
        "            col = 0\n",
        "            for j in range(n_channels):\n",
        "                segments[dim].append([])\n",
        "                segments[dim][j].append(quat.iloc[i:i+n_time_steps,col:col+n_columns].values)\n",
        "                col+=n_columns\n",
        "            labels.append(label-1)\n",
        "            dim+=1\n",
        "\n",
        "\n",
        "    del quat\n",
        "\n",
        "    return segments, labels, dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R53ky8syWzup"
      },
      "outputs": [],
      "source": [
        "def load_quat(path, lim, train, n_channels):\n",
        "\n",
        "    step = n_time_steps//4  # n_time_steps/50 segundos de actividad y pasos de step/50 segundos (de overlap)\n",
        "\n",
        "    # Con listas\n",
        "    segments = []\n",
        "    labels = []\n",
        "\n",
        "    i=1\n",
        "    dim = 0\n",
        "    for filename in path:\n",
        "        print(\"Reading %s (%d/%d)                                                   \"%(filename, i, len(path)), end='\\r')\n",
        "        df = pd.read_csv(filename,sep=',',names=[\"QUAT\",\"w\",\"x\",\"y\",\"z\",\"timestamp\"])\n",
        "        label = int(filename[-6:-4])\n",
        "        for k in range(len(activities)):\n",
        "            if(label==activities[k]):\n",
        "                label=k+1\n",
        "\n",
        "        segments, labels, dim = sliding_window(df.iloc[:lim], n_time_steps, step, segments, labels, label, dim, train, n_channels)\n",
        "\n",
        "        i+=1\n",
        "\n",
        "    del df\n",
        "\n",
        "    return segments, labels\n",
        "\n",
        "\n",
        "def load_train_quat(filename, lim, n_channels):\n",
        "    return load_quat(filename, lim, True, n_channels)\n",
        "\n",
        "def load_test_quat(filename, lim, n_channels):\n",
        "    return load_quat(filename, lim, False, n_channels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sq85HK6SWzup"
      },
      "outputs": [],
      "source": [
        "def get_dataset(data_path, lim, batch_size, n_time_steps, train, valid, ds, n_channels):\n",
        "    if(train):\n",
        "        segments, labels = load_train_quat(data_path, lim, n_channels)\n",
        "    else:\n",
        "        segments, labels = load_test_quat(data_path, lim, n_channels)\n",
        "\n",
        "    print('Generating the dataset                                                   ')\n",
        "\n",
        "    array = np.asarray(segments, dtype = 'float32')\n",
        "    segments = np.reshape(array, (array.shape[0], n_channels, n_time_steps, int(20/n_channels)))\n",
        "    array = np.asarray(labels, dtype = 'int8')\n",
        "    labels = np.reshape(array, (array.shape[0], 1))\n",
        "\n",
        "    del array\n",
        "\n",
        "    # Map coninous dataset to categorical (One-Hot)\n",
        "    labels = keras.utils.to_categorical(labels, len(activities))\n",
        "\n",
        "    if(train):\n",
        "        print('-'*20 + 'TRAIN' + '-'*20)\n",
        "    elif(valid):\n",
        "        print('-'*18 + 'VALIDATION' + '-'*17)\n",
        "    else:\n",
        "        print('-'*20 + 'TEST' + '-'*21)\n",
        "\n",
        "    if(ds):\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((segments, labels))\n",
        "\n",
        "        # It's necessary to repeat our data for all epochs\n",
        "        dataset = dataset.batch(batch_size)\n",
        "\n",
        "        dataset = dataset.shuffle(segments.shape[0])\n",
        "\n",
        "        print('Dataset generated                                                        ')\n",
        "\n",
        "        return dataset, segments, labels\n",
        "    else:\n",
        "        # Shuffle in the first dimension\n",
        "        np.random.seed(235)\n",
        "        permutation = np.arange(0,segments.shape[0]-1)\n",
        "        np.random.shuffle(permutation)\n",
        "        segments = segments[permutation]\n",
        "        labels = labels[permutation]\n",
        "        print('Dataset generated                                                        ')\n",
        "\n",
        "        return segments, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gitX_iBAWzup"
      },
      "source": [
        "### Vector-normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xFBtaImWzup"
      },
      "outputs": [],
      "source": [
        "def normalize(seg):\n",
        "    for i in range(seg.shape[0]):\n",
        "        for j in range(seg.shape[2]):\n",
        "            seg[i,0,j,:] = seg[i,0,j,:]/np.linalg.norm(seg[i,0,j,:])\n",
        "    return seg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpoKJAYDWzup"
      },
      "source": [
        "### Mean removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyYZ2u8PWzup"
      },
      "outputs": [],
      "source": [
        "def mean_removal(seg):\n",
        "    for i in range(seg.shape[0]):\n",
        "        for j in range(seg.shape[2]):\n",
        "            seg[i,0,j,:] -= np.mean(seg[i,0,j,:])\n",
        "    return seg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gfNZx1mWzup"
      },
      "source": [
        "### FFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB066ArsWzux"
      },
      "outputs": [],
      "source": [
        "\n",
        "def fft(seg):\n",
        "    fft0 = np.abs(np.fft.fft2(seg[0,0],norm=\"ortho\"))\n",
        "    k=0\n",
        "    for i in range(1,seg.shape[0]//10):\n",
        "        print('Calculating 2D FFT... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        fft = np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))\n",
        "        fft0 = np.concatenate((fft0,fft),axis=0)\n",
        "        k=i\n",
        "\n",
        "    fft1 = np.abs(np.fft.fft2(seg[k+1,0], norm=\"ortho\"))\n",
        "    for i in range(k+2,2*seg.shape[0]//10):\n",
        "        print('Calculating 2D FFT... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        fft = np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))\n",
        "        fft1 = np.concatenate((fft1,fft),axis=0)\n",
        "        k=i\n",
        "\n",
        "    fft2 = np.abs(np.fft.fft2(seg[k+1,0],norm=\"ortho\"))\n",
        "    for i in range(k+2,3*seg.shape[0]//10):\n",
        "        print('Calculating 2D FFT... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        fft = np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))\n",
        "        fft2 = np.concatenate((fft2,fft),axis=0)\n",
        "        k=i\n",
        "\n",
        "    fft3 = np.abs(np.fft.fft2(seg[k+1,0],norm=\"ortho\"))\n",
        "    for i in range(k+2,4*seg.shape[0]//10):\n",
        "        print('Calculating 2D FFT... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        fft = np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))\n",
        "        fft3 = np.concatenate((fft3,fft),axis=0)\n",
        "        k=i\n",
        "\n",
        "    fft4 = np.abs(np.fft.fft2(seg[k+1,0],norm=\"ortho\"))\n",
        "    for i in range(k+2,5*seg.shape[0]//10):\n",
        "        print('Calculating 2D FFT... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        fft = np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))\n",
        "        fft4 = np.concatenate((fft4,fft),axis=0)\n",
        "        k=i\n",
        "\n",
        "    fft5 = np.abs(np.fft.fft2(seg[k+1,0],norm=\"ortho\"))\n",
        "    for i in range(k+2,6*seg.shape[0]//10):\n",
        "        print('Calculating 2D FFT... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        fft = np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))\n",
        "        fft5 = np.concatenate((fft5,fft),axis=0)\n",
        "        k=i\n",
        "\n",
        "    fft6 =np.abs(np.fft.fft2(seg[k+1,0],norm=\"ortho\"))\n",
        "    for i in range(k+2,7*seg.shape[0]//10):\n",
        "        print('Calculating 2D FFT... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        fft = np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))\n",
        "        fft6 = np.concatenate((fft6,fft),axis=0)\n",
        "        k=i\n",
        "\n",
        "    fft7 = np.abs(np.fft.fft2(seg[k+1,0],norm=\"ortho\"))\n",
        "    for i in range(k+2,8*seg.shape[0]//10):\n",
        "        print('Calculating 2D FFT... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        fft = np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))\n",
        "        fft7 = np.concatenate((fft7,fft),axis=0)\n",
        "        k=i\n",
        "\n",
        "    fft8 =np.abs( np.fft.fft2(seg[k+1,0],norm=\"ortho\"))\n",
        "    for i in range(k+2,9*seg.shape[0]//10):\n",
        "        print('Calculating 2D FFT... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        fft = np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))\n",
        "        fft8 = np.concatenate((fft8,fft),axis=0)\n",
        "        k=i\n",
        "\n",
        "    fft9 = np.abs(np.fft.fft2(seg[k+1,0],norm=\"ortho\"))\n",
        "    for i in range(k+2,seg.shape[0]):\n",
        "        print('Calculating 2D FFT... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        fft = np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))\n",
        "        fft9 = np.concatenate((fft9,fft),axis=0)\n",
        "\n",
        "    return  np.concatenate((fft0,fft1,fft2,fft3,fft4,fft5,fft6,fft7,fft8,fft9), axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm_cVpkbWzux"
      },
      "source": [
        "# POWER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRhwfWhoWzux"
      },
      "outputs": [],
      "source": [
        "\n",
        "def pot(seg):\n",
        "    pot0 = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[0,0],norm=\"ortho\"))**2))\n",
        "    k=0\n",
        "    for i in range(1,seg.shape[0]//10):\n",
        "        print('Calculating 2D pot... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        pot = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))**2))\n",
        "        pot0 = np.concatenate((pot0,pot),axis=0)\n",
        "        k=i\n",
        "\n",
        "    pot1 = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[k+1,0], norm=\"ortho\"))**2))\n",
        "    for i in range(k+2,2*seg.shape[0]//10):\n",
        "        print('Calculating 2D pot... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        pot = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))**2))\n",
        "        pot1 = np.concatenate((pot1,pot),axis=0)\n",
        "        k=i\n",
        "\n",
        "    pot2 = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[k+1,0],norm=\"ortho\"))**2))\n",
        "    for i in range(k+2,3*seg.shape[0]//10):\n",
        "        print('Calculating 2D pot... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        pot = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))**2))\n",
        "        pot2 = np.concatenate((pot2,pot),axis=0)\n",
        "        k=i\n",
        "\n",
        "    pot3 = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[k+1,0],norm=\"ortho\"))**2))\n",
        "    for i in range(k+2,4*seg.shape[0]//10):\n",
        "        print('Calculating 2D pot... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        pot = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))**2))\n",
        "        pot3 = np.concatenate((pot3,pot),axis=0)\n",
        "        k=i\n",
        "\n",
        "    pot4 = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[k+1,0],norm=\"ortho\"))**2))\n",
        "    for i in range(k+2,5*seg.shape[0]//10):\n",
        "        print('Calculating 2D pot... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        pot = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))**2))\n",
        "        pot4 = np.concatenate((pot4,pot),axis=0)\n",
        "        k=i\n",
        "\n",
        "    pot5 = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[k+1,0],norm=\"ortho\"))**2))\n",
        "    for i in range(k+2,6*seg.shape[0]//10):\n",
        "        print('Calculating 2D pot... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        pot = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))**2))\n",
        "        pot5 = np.concatenate((pot5,pot),axis=0)\n",
        "        k=i\n",
        "\n",
        "    pot6 =np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[k+1,0],norm=\"ortho\"))**2))\n",
        "    for i in range(k+2,7*seg.shape[0]//10):\n",
        "        print('Calculating 2D pot... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        pot = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))**2))\n",
        "        pot6 = np.concatenate((pot6,pot),axis=0)\n",
        "        k=i\n",
        "\n",
        "    pot7 = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[k+1,0],norm=\"ortho\"))**2))\n",
        "    for i in range(k+2,8*seg.shape[0]//10):\n",
        "        print('Calculating 2D pot... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        pot = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))**2))\n",
        "        pot7 = np.concatenate((pot7,pot),axis=0)\n",
        "        k=i\n",
        "\n",
        "    pot8 =np.log(np.fft.fftshift(np.abs( np.fft.fft2(seg[k+1,0],norm=\"ortho\"))**2))\n",
        "    for i in range(k+2,9*seg.shape[0]//10):\n",
        "        print('Calculating 2D pot... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        pot = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))**2))\n",
        "        pot8 = np.concatenate((pot8,pot),axis=0)\n",
        "        k=i\n",
        "\n",
        "    pot9 = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[k+1,0],norm=\"ortho\"))**2))\n",
        "    for i in range(k+2,seg.shape[0]):\n",
        "        print('Calculating 2D pot... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        pot = np.log(np.fft.fftshift(np.abs(np.fft.fft2(seg[i,0],norm=\"ortho\"))**2))\n",
        "        pot9 = np.concatenate((pot9,pot),axis=0)\n",
        "\n",
        "    return  np.concatenate((pot0,pot1,pot2,pot3,pot4,pot5,pot6,pot7,pot8,pot9), axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH8304WrWzux"
      },
      "source": [
        "# SPECTROGRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9__O5APuWzux"
      },
      "outputs": [],
      "source": [
        "from scipy import signal\n",
        "def spect(seg):\n",
        "    _,_,spect0 = signal.spectrogram(seg[0,0],50, nperseg=20)\n",
        "    spect0 = spect0[:,:,0]\n",
        "    k=0\n",
        "    for i in range(1,seg.shape[0]//10):\n",
        "        print('Calculating 2D spect... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        _,_,spect = signal.spectrogram(seg[i,0],50, nperseg=20)\n",
        "        spect = spect[:,:,0]\n",
        "        spect0 = np.concatenate((spect0,spect),axis=0)\n",
        "        k=i\n",
        "\n",
        "    _,_,spect1 = signal.spectrogram(seg[0,0],50, nperseg=20)\n",
        "    spect1 = spect1[:,:,0]\n",
        "    for i in range(k+2,2*seg.shape[0]//10):\n",
        "        print('Calculating 2D spect... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        _,_,spect = signal.spectrogram(seg[i,0],50, nperseg=20)\n",
        "        spect = spect[:,:,0]\n",
        "        spect1 = np.concatenate((spect1,spect),axis=0)\n",
        "        k=i\n",
        "\n",
        "    _,_,spect2 = signal.spectrogram(seg[0,0],50, nperseg=20)\n",
        "    spect2 = spect2[:,:,0]\n",
        "    for i in range(k+2,3*seg.shape[0]//10):\n",
        "        print('Calculating 2D spect... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        _,_,spect = signal.spectrogram(seg[i,0],50, nperseg=20)\n",
        "        spect = spect[:,:,0]\n",
        "        spect2 = np.concatenate((spect2,spect),axis=0)\n",
        "        k=i\n",
        "\n",
        "    _,_,spect3 = signal.spectrogram(seg[k+1,0],50, nperseg=20)\n",
        "    spect3 = spect3[:,:,0]\n",
        "    for i in range(k+2,4*seg.shape[0]//10):\n",
        "        print('Calculating 2D spect... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        _,_,spect = signal.spectrogram(seg[i,0],50, nperseg=20)\n",
        "        spect = spect[:,:,0]\n",
        "        spect3 = np.concatenate((spect3,spect),axis=0)\n",
        "        k=i\n",
        "\n",
        "    _,_,spect4 = signal.spectrogram(seg[k+1,0],50, nperseg=20)\n",
        "    spect4 = spect4[:,:,0]\n",
        "    for i in range(k+2,5*seg.shape[0]//10):\n",
        "        print('Calculating 2D FFT... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        _,_,spect = signal.spectrogram(seg[i,0],50, nperseg=20)\n",
        "        spect = spect[:,:,0]\n",
        "        spect4 = np.concatenate((spect4,spect),axis=0)\n",
        "        k=i\n",
        "\n",
        "    _,_,spect5 = signal.spectrogram(seg[k+1,0],50, nperseg=20)\n",
        "    spect5 = spect5[:,:,0]\n",
        "    for i in range(k+2,6*seg.shape[0]//10):\n",
        "        print('Calculating 2D spect... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        _,_,spect = signal.spectrogram(seg[i,0],50, nperseg=20)\n",
        "        spect = spect[:,:,0]\n",
        "        spect5 = np.concatenate((spect5,spect),axis=0)\n",
        "        k=i\n",
        "\n",
        "    _,_,spect6 = signal.spectrogram(seg[k+1,0],50, nperseg=20)\n",
        "    spect6 = spect6[:,:,0]\n",
        "    for i in range(k+2,7*seg.shape[0]//10):\n",
        "        print('Calculating 2D spect... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        _,_,spect = signal.spectrogram(seg[i,0],50, nperseg=20)\n",
        "        spect = spect[:,:,0]\n",
        "        spect6 = np.concatenate((spect6,spect),axis=0)\n",
        "        k=i\n",
        "\n",
        "    _,_,spect7 = signal.spectrogram(seg[k+1,0],50, nperseg=20)\n",
        "    spect7 = spect7[:,:,0]\n",
        "    for i in range(k+2,8*seg.shape[0]//10):\n",
        "        print('Calculating 2D spect... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        _,_,spect = signal.spectrogram(seg[i,0],50, nperseg=20)\n",
        "        spect = spect[:,:,0]\n",
        "        spect7 = np.concatenate((spect7,spect),axis=0)\n",
        "        k=i\n",
        "\n",
        "    _,_,spect8 = signal.spectrogram(seg[k+1,0],50, nperseg=20)\n",
        "    spect8 = spect8[:,:,0]\n",
        "    for i in range(k+2,9*seg.shape[0]//10):\n",
        "        print('Calculating 2D spect... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        _,_,spect = signal.spectrogram(seg[i,0],50, nperseg=20)\n",
        "        spect = spect[:,:,0]\n",
        "        spect8 = np.concatenate((spect8,spect),axis=0)\n",
        "        k=i\n",
        "\n",
        "    _,_,spect9 = signal.spectrogram(seg[k+1,0],50, nperseg=20)\n",
        "    spect9 = spect9[:,:,0]\n",
        "    for i in range(k+2,seg.shape[0]):\n",
        "        print('Calculating 2D spect... (%d/%d)'%(i+1,seg.shape[0]), end='\\r')\n",
        "        _,_,spect = signal.spectrogram(seg[i,0],50, nperseg=20)\n",
        "        spect = spect[:,:,0]\n",
        "        spect9 = np.concatenate((spect9,spect),axis=0)\n",
        "\n",
        "    return  np.concatenate((spect0,spect1,spect2,spect3,spect4,spect5,spect6,spect7,spect8,spect9), axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNtqlgljWzux"
      },
      "outputs": [],
      "source": [
        "global n_time_steps\n",
        "n_time_steps = 128\n",
        "\n",
        "def generator(s):\n",
        "    dataset = False # Set to True if you want a dataset or to False if you want np.arrays\n",
        "    batch_size = 32 # REAL batch_size\n",
        "    n_channels = 1 # It can be 1,4 or 9\n",
        "\n",
        "    subj = s[0][41:43]\n",
        "\n",
        "    seg, lab = get_dataset(s, ind_tr, batch_size, n_time_steps, True, False, dataset, n_channels)\n",
        "    print(seg.shape, lab.shape)\n",
        "\n",
        "    # Norm before features\n",
        "    seg = normalize(seg)\n",
        "\n",
        "    '''\n",
        "    # Mean removal\n",
        "    seg = mean_removal(seg)\n",
        "    '''\n",
        "\n",
        "    # FFT\n",
        "    data_fft = fft(seg)\n",
        "    data_fft = np.reshape(data_fft,(seg.shape[0], seg.shape[1], seg.shape[2], seg.shape[3]))\n",
        "    print(data_fft.shape)\n",
        "\n",
        "    '''\n",
        "    # POWER\n",
        "    data_pot = pot(seg)\n",
        "    data_pot = np.reshape(data_pot,(seg.shape[0], seg.shape[1], seg.shape[2], seg.shape[3]))\n",
        "    print(data_pot.shape)\n",
        "    '''\n",
        "    '''\n",
        "    # SPECTROGRAM\n",
        "    data_spect = spect(seg)\n",
        "    data_spect = np.reshape(data_spect,(seg.shape[0], seg.shape[1], seg.shape[2], 11))\n",
        "    print(data_spect.shape)\n",
        "    '''\n",
        "\n",
        "    seg = np.concatenate((seg,data_fft),axis=3)\n",
        "    del data_fft\n",
        "    '''\n",
        "    seg = np.concatenate((seg,data_pot),axis=3)\n",
        "    del data_pot\n",
        "\n",
        "    seg = np.concatenate((seg,data_spect),axis=3)\n",
        "    del data_spect\n",
        "\n",
        "    '''\n",
        "    '''\n",
        "    # Norm after features\n",
        "    seg = normalize(seg)\n",
        "    '''\n",
        "\n",
        "    print(seg.shape, lab.shape)\n",
        "\n",
        "    np.save('./defNORM_raworientation/subject_video_'+subj+'_seg.npy', seg)\n",
        "    np.save('./defNORM_raworientation/subject_video_'+subj+'_lab.npy', lab)\n",
        "\n",
        "    print('Files from subject ' + subj + ' saved')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmIoopcrWzux",
        "outputId": "e10e653e-b5cd-4fb8-c7f4-6cd1441afe3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating the dataset                                                   (11/11)                                                   \n",
            "--------------------TRAIN--------------------\n",
            "Dataset generated                                                        \n",
            "(98, 1, 128, 20) (98, 11)\n",
            "(98, 1, 128, 20)FT... (98/98)\n",
            "(98, 1, 128, 40) (98, 11)\n",
            "Files from subject 01 saved\n"
          ]
        }
      ],
      "source": [
        "generator(s1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7pzkGfVWzuy",
        "outputId": "617acf70-61b7-4e2d-d8d8-a4ac3515956c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating the dataset                                                   (11/11)                                                   \n",
            "--------------------TRAIN--------------------\n",
            "Dataset generated                                                        \n",
            "(98, 1, 128, 20) (98, 11)\n",
            "(98, 1, 128, 20)FT... (98/98)\n",
            "(98, 1, 128, 40) (98, 11)\n",
            "Files from subject 02 saved\n"
          ]
        }
      ],
      "source": [
        "generator(s2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQxHBChSWzuy",
        "outputId": "87f523e8-8ed8-464c-a766-7df04a5acdb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating the dataset                                                   (11/11)                                                   \n",
            "--------------------TRAIN--------------------\n",
            "Dataset generated                                                        \n",
            "(98, 1, 128, 20) (98, 11)\n",
            "(98, 1, 128, 20)FT... (98/98)\n",
            "(98, 1, 128, 40) (98, 11)\n",
            "Files from subject 03 saved\n"
          ]
        }
      ],
      "source": [
        "generator(s3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BywMtXs4Wzuy",
        "outputId": "fb6a9547-015a-43e1-c3e6-19cdd69a2058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating the dataset                                                   (11/11)                                                   \n",
            "--------------------TRAIN--------------------\n",
            "Dataset generated                                                        \n",
            "(98, 1, 128, 20) (98, 11)\n",
            "(98, 1, 128, 20)FT... (98/98)\n",
            "(98, 1, 128, 40) (98, 11)\n",
            "Files from subject 05 saved\n"
          ]
        }
      ],
      "source": [
        "generator(s5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWFwUIyVWzuy",
        "outputId": "ec1842a5-8b11-43fb-ffba-7b946270a4c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating the dataset                                                   (11/11)                                                   \n",
            "--------------------TRAIN--------------------\n",
            "Dataset generated                                                        \n",
            "(98, 1, 128, 20) (98, 11)\n",
            "(98, 1, 128, 20)FT... (98/98)\n",
            "(98, 1, 128, 40) (98, 11)\n",
            "Files from subject 08 saved\n"
          ]
        }
      ],
      "source": [
        "generator(s8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syT70hn8Wzuy",
        "outputId": "1f116056-cd20-47a6-dd08-d4095fb1d76a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating the dataset                                                   (11/11)                                                   \n",
            "--------------------TRAIN--------------------\n",
            "Dataset generated                                                        \n",
            "(98, 1, 128, 20) (98, 11)\n",
            "(98, 1, 128, 20)FT... (98/98)\n",
            "(98, 1, 128, 40) (98, 11)\n",
            "Files from subject 09 saved\n"
          ]
        }
      ],
      "source": [
        "generator(s9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmAIAA3PWzuy",
        "outputId": "89f98090-b2ba-40ab-cf12-c37e87604951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating the dataset                                                   (11/11)                                                   \n",
            "--------------------TRAIN--------------------\n",
            "Dataset generated                                                        \n",
            "(98, 1, 128, 20) (98, 11)\n",
            "(98, 1, 128, 20)FT... (98/98)\n",
            "(98, 1, 128, 40) (98, 11)\n",
            "Files from subject 10 saved\n"
          ]
        }
      ],
      "source": [
        "generator(s10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJqb0J-MWzuy",
        "outputId": "794550a2-f082-4bbd-8051-5bd3b772b924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating the dataset                                                   (11/11)                                                   \n",
            "--------------------TRAIN--------------------\n",
            "Dataset generated                                                        \n",
            "(98, 1, 128, 20) (98, 11)\n",
            "(98, 1, 128, 20)FT... (98/98)\n",
            "(98, 1, 128, 40) (98, 11)\n",
            "Files from subject 11 saved\n"
          ]
        }
      ],
      "source": [
        "generator(s11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXOgTSKrWzuy",
        "outputId": "89a3509c-922d-4bed-f265-15ea61560978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating the dataset                                                   (11/11)                                                   \n",
            "--------------------TRAIN--------------------\n",
            "Dataset generated                                                        \n",
            "(98, 1, 128, 20) (98, 11)\n",
            "(98, 1, 128, 20)FT... (98/98)\n",
            "(98, 1, 128, 40) (98, 11)\n",
            "Files from subject 13 saved\n"
          ]
        }
      ],
      "source": [
        "generator(s13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yUcuO3mWzuy",
        "outputId": "543a2e1f-0e4b-4f1d-ade5-d05afe0b3c2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating the dataset                                                   (11/11)                                                   \n",
            "--------------------TRAIN--------------------\n",
            "Dataset generated                                                        \n",
            "(98, 1, 128, 20) (98, 11)\n",
            "(98, 1, 128, 20)FT... (98/98)\n",
            "(98, 1, 128, 40) (98, 11)\n",
            "Files from subject 14 saved\n"
          ]
        }
      ],
      "source": [
        "generator(s14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kllRZLG_Wzuy",
        "outputId": "760f3922-8ba3-447b-847f-c6ab7f73c9b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating the dataset                                                   (11/11)                                                   \n",
            "--------------------TRAIN--------------------\n",
            "Dataset generated                                                        \n",
            "(98, 1, 128, 20) (98, 11)\n",
            "(98, 1, 128, 20)FT... (98/98)\n",
            "(98, 1, 128, 40) (98, 11)\n",
            "Files from subject 16 saved\n"
          ]
        }
      ],
      "source": [
        "generator(s16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TognsxYWzuy",
        "outputId": "a1c81da1-27a9-408c-8724-c5f2ab0922aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating the dataset                                                   (11/11)                                                   \n",
            "--------------------TRAIN--------------------\n",
            "Dataset generated                                                        \n",
            "(98, 1, 128, 20) (98, 11)\n",
            "(98, 1, 128, 20)FT... (98/98)\n",
            "(98, 1, 128, 40) (98, 11)\n",
            "Files from subject 17 saved\n"
          ]
        }
      ],
      "source": [
        "generator(s17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQx1Yt4HWzuy",
        "outputId": "316757c8-b6d5-488c-d68d-21f724683c01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OperaciÃ³n terminada.\n"
          ]
        }
      ],
      "source": [
        "print('OperaciÃ³n terminada.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}